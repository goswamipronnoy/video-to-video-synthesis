<!DOCTYPE html>

<html lang="en">

<head>

  <!-- Basic Page Needs
  ================================================== -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116539120-3"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-116539120-3');
  </script>

  <meta charset="utf-8">
  <title>Video to Video Synthesis</title>
  <meta name="description" content="Video to Video Synthesis">
  <!-- <meta name="author" content="CVL"> -->

  <!-- Mobile Specific Metas
	================================================== -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="css/materialize.min.css" media="screen,projection" />

  <!-- CSS
	================================================== -->
  <link rel="stylesheet" href="css/zerogrid.css">
  <link rel="stylesheet" href="css/style.css">

  <!-- Custom Fonts -->
  <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300" rel="stylesheet">


  <link rel="stylesheet" href="css/menu.css">
  <script src="js/jquery1111.min.js" type="text/javascript"></script>
  <script src="js/script.js"></script>

  <script type="text/javascript" src="js/materialize.min.js"></script>

  <!-- Add smooth transition -->
  <script>
    $(document).ready(function () {
      // Add smooth scrolling to all links
      $("a").on('click', function (event) {

        // Make sure this.hash has a value before overriding default behavior
        if (this.hash !== "") {
          // Prevent default anchor click behavior
          event.preventDefault();

          // Store hash
          var hash = this.hash;

          // Using jQuery's animate() method to add smooth page scroll
          // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
          $('html, body').animate({
            scrollTop: $(hash).offset().top
          }, 800, function () {

            // Add hash (#) to URL when done scrolling (default click behavior)
            window.location.hash = hash;
          });
        } // End if
      });
    });
  </script>

  <style type="text/css">
    /*********************************
    The list of publication items
    *********************************/
    /* The list of items */
    .biblist {}

    /* The item */
    .biblist li {}

    /* You can define custom styles for plstyle field here. */


    /*************************************
     The box that contain BibTeX code
     *************************************/
    div.noshow {
      display: none;
    }

    div.bibtex {
      margin-right: 0%;
      margin-top: 1.2em;
      margin-bottom: 1.3em;
      border: 1px solid silver;
      padding: 0.3em 0.5em;
      background: #ffffee;
    }

    div.bibtex pre {
      font-size: 75%;
      overflow: auto;
      width: 100%;
    }


    .content-subhead {
      color: black
    }

    .btn-large {
      text-transform: none;
      background-color: #AAAAAA;
    }

    .round {
      width: 100%;
      padding-top: 100%;
      overflow: hidden;
      position: relative;
      border-radius: 50%;
    }

    img.roundimg {
      position: absolute;
      top: 50%;
      left: 50%;
      min-width: 100%;
      height: 100%;
      transform: translate(-50%, -50%);
      transition: 1s ease;
    }
  </style>

  <script type="text/javascript">
 < !--
      // Toggle Display of BibTeX
      function toggleBibtex(articleid) {
        var bib = document.getElementById('bib_' + articleid);
        if (bib) {
          if (bib.className.indexOf('bibtex') != -1) {
            bib.className.indexOf('noshow') == -1 ? bib.className = 'bibtex noshow' : bib.className = 'bibtex';
          }
        } else {
          return;
        }
      }
    -->
  </script>
</head>

<body>
  <div class="wrap-body">

    <!--////////////////////////////////////Header-->
    <header class="zerogrid">
      <div id='cssmenu' class="align-center">
        <ul>
          <li class="active"><a href='index.html'><span>Home</span></a></li>
          <li><a href='#about'><span>About</span></a></li>
          <li><a href='#problemFormulation'><span>Problem Formulation</span></a></li>
          <li><a href='#researchMethodology'><span>Research Methodology</span></a></li>
          <li><a href='#results'><span>Results</span></a></li>
          <li><a href='#analysis'><span>Analysis and Benchmarks</span></a></li>
          <li><a href='#team'><span>Team</span></a></li>
          <li><a href='#datasets'><span>Datasets</span></a></li>
          <li><a href='#timeline'><span>Timeline</span></a></li>
          <li><a href='#references'><span>References</span></a></li>
        </ul>
      </div>
    </header>


    <!--////////////////////////////////////Container-->
    <section id="container">
      <div class="wrap-container">
        <!-----------------content-box-2-------------------->
        <section class="content-box box-2">
          <div class="zerogrid">
            <div class="row wrap-box">
              <!--Start Box-->
              <div class="header">
                <hr class="line-2">
                <h1>Video to Video Synthesis</h1>
                <span style="line-height: 1.2">Our project aims to implement NVIDIA's Video-to-Video Synthesis research
                  paper, the goal of the research is to learn a mapping function from an input source video
                  (sequences of semantic segmentation masks) to an output photo-realistic video that precisely depicts
                  the content of the source video.</span>
              </div>
              <div class="row">
                <div class="col-1-2 t-center">
                  <img src="images/vid-gifs/cityscapes_change_labels.gif" style="width:98%" />
                </div>
                <div class="offsetonlyonmobile"></div>
                <div class="col-1-2 t-center">
                  <img src="images/vid-gifs/cityscapes_change_styles.gif" style="width:98%" />
                </div>
              </div>
              <article>
                <div class="row">


                  <a href="https://github.com/sakshamgupta006/Awesome_Video_Object_Segmentation"
                    style="padding-right: 2em"><img src="./images/github.png" alt="slides" width="
                  30" height="60"></img>Repository </a>
                  <a href="https://docs.google.com/presentation/d/1p7FVbR2tJ86ROuZO9RSLGC6lg06aDLjmBMRPKsLu2F0/edit?usp=sharing"
                    style="padding-right: 2em"><img src="./images/google_slides.png" alt="slides" width="30"
                      height="60"></img> Google Slides </a>
                  <a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf" style="padding-right: 2em"><img
                      src="./images/cvf.jpg" alt="slides" width="30" height="60"></img> Paper </a>
                </div>
              </article>
            </div>
          </div>
        </section>
      </div>
    </section>





    <section class="content-box box-3" id="about">
      <div class="zerogrid">
        <article>
          <div class="row t-center" style="line-height: 1.5">
            <hr class="line-2">
            <h2 style="margin-bottom:20px">About</h2>
            <div>
              <div style="text-align:justify; padding-top: 1em; padding-bottom: 2em;">
                <span>
                  The capability to model and recreate the dynamics of our visual world is essential to building
                  intelligent agents.
                  Apart from purely scientific interests, learning to synthesize continuous visual experiences has a
                  wide range of
                  applications in computer vision, robotics, and computer graphics. For example, in model-based
                  reinforcement learning
                  , a video synthesis model finds use in approximating visual dynamics of the world for training the
                  agent with less
                  amount of real experience data. Using a learned video synthesis model, one can generate realistic
                  videos without
                  explicitly specifying scene geometry, materials, lighting, and their dynamics, which would be
                  cumbersome but necessary
                  when using standard graphics rendering techniques.
                </span>
              </div>
              <div style="text-align:left">
                <h2 style="margin-bottom:20px; text-align:left">Motivation "TODO"</h2>
                <ul>
                  <li>1. Availability of Large Datasets [DAVIS, Youtube-VOS].</li>
                  <li>2. Availability of fast compute resources [GPU’s].</li>
                  <li>3. Vast number of applications [Video Editing, Augmented Reality].</li>
                </ul>
              </div>
            </div>
          </div>
        </article>
      </div>
  </div>
  </section>

  <section class="content-box box-3" id="problemFormulation">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Problem Formulation</h2>
          <div style="text-align:justify">
            <div><b>Given: </b>Aligned input and output videos.</div>
            <div>
              <b>Task: </b>Learn to map input videos to the output domain at the test time:
              <div style="text-align: justify; padding-left: 2.5em">
                <ul>
                  <li>1. Carefully designed Generator and Discriminator Networks.</li>
                  <li>2. Novel spatio-temporal learning objective function.</li>
                  <li>3. Method to multimodal video synthesis.</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <section class="content-box box-3" id="researchMethodology">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Research Methodology</h2>
          <div style="text-align:justify">
            <div class="zerogrid">
              <article>
                <div style="line-height: 1">
                  <hr class="line-3">
                  <h3 style="margin-bottom:20px; padding-top: 1em; ">1. Recap: Generative Adversarial Networks</h3>
                  <div style="text-align:justify">
                    <div class="row" style="text-align:center; padding-top:2em">
                      <img width="600" height="400" src="images/GAN.png"></img>
                      <figcaption style="padding-top: 1.5em"><b>Figure: A typical Generative Adversarial Networks
                          Pipeline</b></figcaption>
                      <div style="padding-top: 2em">
                        <div class="row" style="text-align:center; padding-top:2em">
                          <img width="600" height="400" src="images/Gan_eq.png"></img>
                          <div style="padding-top: 2em">
                          </div>
                        </div>
              </article>
              <article>
                <div style="line-height: 1">
                  <hr class="line-3">
                  <h3 style="margin-bottom:20px; padding-top: 1em; ">2. Recap: Conditional Generative Adversarial
                    Networks</h3>
                  <div style="text-align:justify">
                    <div class="row" style="text-align:center; padding-top:2em">
                      <img width="600" height="400" src="images/conditionalGAN.png"></img>
                      <figcaption style="padding-top: 1.5em"><b>Figure: A typical Conditional Generative Adversarial
                          Networks Pipeline</b></figcaption>
                      <div style="padding-top: 2em">
                        <div class="row" style="text-align:center; padding-top:2em">
                          <img width="600" height="400" src="images/ConditionalGan_eq.png"></img>
                          <div style="padding-top: 2em">
                          </div>
                        </div>
              </article>
            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <section class="content-box box-3" id="approach">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Approach</h2>
          <div style="text-align:justify">
            Since a lot of new research in the field of Video Object Segmentation (VOS) is taking place continuously, so
            we decided to create
            a standard Github repository Awesome_Video_Object_Segmentation, which will serve as a go-to repo for
            establishing previous baselines
            on different datasets and models. Our main function take arguments like approach, dataset, models, weights
            etc. to produce accuracy scores and metrics so that easy comparison can be facilitated.
          </div>
          <div class="row" style="text-align:center; padding-top:2em">
            <img width="800" height="420" src="images/plug_n_play.png"></img>
            <figcaption>Figure: Plug 'n' Play Pipleline</figcaption>
            <div style="padding-top: 2em">
              <h2><b>Approaches Integrated</b></h2>
            </div>
            <div>
            </div>

            <div>
              <div style="text-align: justify; padding-top: 1.5em">
                <h2 style="margin-bottom:20px">1. OSVOS: One-Shot Video Object Segmentation</h2>
                This research paper deals with the task of semi-supervised video object segmentation problem. It
                proposes One-Shot Video Segmentation method based on a fully-convolutional neural network which trans-
                fers the semantic information learned on ImageNet to solve the problem of background and foreground
                segmentation. It adapts a CNN that is pre-trained on image recognition for video object segmentation.
                Finally, it is fine-tuned on a particular object that is manually segmented in a frame.
                <div style="padding-right: 2em; padding-top:1em; text-align:center">
                  <img width="420" height="420" src="images/OSVOS.png"></img>
                  <figcaption>Figure: OSVOS: One-Shot Video Object Segmentation</figcaption>
                </div>
              </div>

              <div style="text-align: justify; padding-top: 1.5em">
                <h2 style="margin-bottom:20px">2. MaskTrack: Learning Video Object Segmentation from static images</h2>
                This research paper proposes a approach to solve the problem of VOS as a guided instance segmentation.
                It uses existing semantic labelling ConvNets and trains them for object segmentation for each frame of
                the video. It further solves the problem of instance level recognition using (a.) offline training,
                which guides the network towards the region of interest obtained from the previous frame mask, and (b.)
                online training, fine-tuning of attributes so that, the model learns the specific instance properly.
                Also, this method is agnostic of the annotation method making the system robust for various
                applications.
                <div style="padding-right: 2em; padding-top:1em; text-align:center">
                  <img width="420" height="420" src="images/MaskTrack.png"></img>
                  <figcaption style="padding-top:0.5em">Figure: MaskTrack</figcaption>
                </div>
              </div>

              <div style="text-align: justify; padding-top: 1.5em">
                <h2 style="margin-bottom:20px">3. VS-ReID: Video Object Segmentation with Re-identification</h2>
                <p>This research paper presents a framework (VS-ReID) to over- come the challenges of temporal
                  continuity while propagating masks between the frames. These being (a.) drifting, and (b.) inabil- ity
                  to handle large displacements which are solved by preventing the target from being lost using adaptive
                  object re-identification.</p>
                <p>The VS-ReID model consists of two modules. Firstly, a mask propagation module which transfers the
                  probability map of the pre- dicted frames to the neighboring frames. Secondly, a reidentification
                  module which retrieves the missing instances during the mask prop- agation process. After this, the
                  work proposes an algorithm called VS-ReID that iteratively combines the previous two masks for all the
                  instances of the video.</p>

                <div style="padding-right: 2em; padding-top:1em; text-align:center">
                  <img width="420" height="420" src="images/vsreid.png"></img>
                  <figcaption style="padding-top:0.5em">Figure: VS-ReID: Video Object Segmentation with
                    Re-identification</figcaption>
                </div>
              </div>

              <div style="text-align: justify; padding-top: 1.5em">
                <h2 style="margin-bottom:20px">4. MaskRNN: Instance Level Video Object Segmentation</h2>
                <p>This paper uses recurrent neural networks motivated by the fact that frames are time-dependent in a
                  video sequence. Since, the frames are time-dependent the prediction of the previous frame influences
                  the prediction of the current frame. Then an optical flow is computed between the two frames. Further,
                  this learned optical flow is fed to the N deep neural networks, one for each object, to warp the
                  previous prediction to align with the current frame in the video.</p>
                <p>Each deep neural network consists of two parts: (a.) a binary seg- mentation net, which predicts the
                  segmentation mask to be applied and (b.) an object localization net, which performs the bounding box
                  regression. Once, the segmentation mask and the bounding box regression are obtained they are further
                  merged to create a binary segmentation mask. These binary segmentation’s are merged for each of the N
                  objects at the time of testing using the argmax operation.</p>
                <div style="padding-right: 2em; padding-top:1em; text-align:center">
                  <img width="420" height="420" src="images/MaskRNN.png"></img>
                  <figcaption style="padding-top:0.5em">Figure: MaskRNN</figcaption>
                </div>
              </div>
            </div>
          </div>
      </article>
    </div>
    </div>
  </section>
  <!--TEAM-->
  <section class="content-box box-3" id="team">
    <div class="zerogrid">
      <div class="row wrap-box t-center" style="margin-left:10%;margin-right:10%;">
        <!--Start Box-->
        <hr class="line-2">
        <h2 style="margin-bottom:20px">Team</h2>
        <article>
          <div class="row">
            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/guptasaksham/" target="_blank"><img src="images/saksham.jpg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/guptasaksham/" target="_blank">
                    <h3 style="margin: 20px;">Saksham Gupta</h3>
                  </a>
                </div>
              </div>
            </div>

            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/pronnoygoswami/" target="_blank"><img src="images/pronnoy.jpeg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/pronnoygoswami/" target="_blank">
                    <h3 style="margin: 20px;">Pronnoy Goswami</h3>
                  </a>
                </div>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/parth-vora/" target="_blank"><img src="images/parth.jpg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/parth-vora/" target="_blank">
                    <h3 style="margin: 20px;">Parth Vora</h3>
                  </a>
                </div>
              </div>
            </div>

            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/mohak-bheda/" target="_blank"><img src="images/mohak.jpg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/mohak-bheda/" target="_blank">
                    <h3 style="margin: 20px;">Mohak Bheda</h3>
                  </a>
                </div>
              </div>
            </div>
          </div>
        </article>
      </div>
    </div>
  </section>

  <!--RESULTS SECTION-->
  <section class="content-box box-3" id="results">
    <div class="zerogrid">
      <div class="row wrap-box t-center" style="margin-left:10%;margin-right:10%;">
        <!--Start Box-->
        <hr class="line-2">
        <h2 style="margin-bottom:20px">Results</h2>
        <div class="row">
          <div class="col-1-5 t-center">
            <img src="images/1.jpg" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/2.jpg" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/16.jpg" style="width:98%" />
          </div>
          <div class="offsetonlyonmobile">
          </div>
          <div class="col-1-5 t-center">
            <img src="images/20.jpg" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/30.jpg" style="width:98%" />
          </div>
        </div>

        <div class="row">
          <div class="col-1-5 t-center">
            <img src="images/real_1.png" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_2.png" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_16.png" style="width:98%" />
          </div>
          <div class="offsetonlyonmobile">
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_20.png" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_30.png" style="width:98%" />
          </div>
        </div>
        <div>Original Frames and their Segmentation Masks</div>
      </div>
    </div>
  </section>

  <!-- ANALYSIS AND BENCHMARKS -->
  <section class="content-box box-3" id="analysis">
    <div class="zerogrid">
      <article>
        <div class="row wrap-box t-center" style="margin-left:10%;margin-right:10%;">
          <!--Start Box-->
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Analysis and Benchmarks</h2>
          <div style="text-align:justify">
            <p>We used two different pre-trained video recognition CNNs in our evaluation: I3D [9] and ResNeXt [15].</p>
            <div style="padding-right: 2em; padding-top:1em; text-align:center">
              <img width="900" height="600" src="images/analysis1.png"></img>
            </div>
            <div style="padding-right: 2em; padding-top:1em; text-align:center">
              <img width="900" height="600" src="images/analysis2.png"></img>
              <figcaption style="padding-top:0.5em">Table: Comparison between competing video-to-video synthesis approaches on Cityscapes.</figcaption>
            </div>
          </div>
        </div>
      </article>
    </div>
  </section>

  <!-- DATASETS -->
  <section class="content-box box-3" id="datasets">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Datasets</h2>
          <div style="text-align:justify">
            <div style="text-align: justify; padding-top: 1.5em">
              <p>The paper uses three different datasets to train their model. We specifically apply a pre-trained
                segmentation algorithm to
                get the corresponding semantic maps on the Cityscapes dataset. We apply a pre-trained segmentation
                algorithm to get the corresponding semantic maps (train_A) and instance maps
                (train_inst). DensePose/OpenPose is applied to their curated dataset consisting of random dancing videos
                found on YouTube to estimate the poses for each frame. We have used Python 3
                and PyTorch for obtaining our segmentation masks.</p>
              <p> The links to the available datasets are given below: </p>
            </div>
          </div>
          <div class="row" style="padding-top: 1.5em">
            <div class="col-1-2 t-center">
              <a class="waves-effect waves-light btn-large" style="width:90%"
                href="https://www.cityscapes-dataset.com/">Cityscapes</a>
            </div>
            <div class="col-1-2 t-center">
              <a class="waves-effect waves-light btn-large" style="width:90%"
                href="http://niessnerlab.org/projects/roessler2018faceforensics.html">FaceForensics</a>
            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <!-- DEMO APPLICATIONS -->
  <section class="content-box box-3" id="training">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Live Training</h2>
          <div class="apps" style="text-align: left">
            <ol>
              <li style="font-weight: bold">
                1. Machine Learned Bokeh Images and Videos
                <div class="row" style="text-align:center; padding-top:2em">
                  <iframe width="500" height="350" src="https://www.youtube.com/embed/un50hRt_9x0" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen
                    style="padding-right: 2em"></iframe>
                  <iframe src="https://drive.google.com/file/d/19XEkWFlmmWMCuN6DZj1bxt2Vtgwetyn4/preview" width="500"
                    height="350"></iframe>
                </div>
              </li>
              <li style="font-weight: bold">
                2. Retro Effect in Images (Greyscaling the background in an image or video)
                <div class="row" style="text-align:center; padding-top:2em">
                  <img width="420" height="420" src="images/girl_1_n.png" style="padding-right: 2em">
                  </img>
                  <img width="420" height="420" src="images/girl_1.png" style="padding-right: 2em">
                  </img>
                </div>
                <div class="row" style="text-align:center; padding-top:2em">
                  <img width="420" height="420" src="images/girl_2_n.png" style="padding-right: 2em">
                  </img>
                  <img width="420" height="420" src="images/girl_2.png" style="padding-right: 2em">
                  </img>
                </div>
              </li>
            </ol>


          </div>

        </div>
      </article>
    </div>
    </div>
  </section>

<!-- TIMELINE -->
  <section class="content-box box-3" id="TimeLine">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Timeline</h2>
          <div style="text-align:justify">
            <div style="padding-right: 2em; padding-top:1em; text-align:center">
              <img width="900" height="600" src="images/timeline.png"></img>
              <figcaption style="padding-top:0.5em">Figure: Time Line of Milestones and Deliverables</figcaption>
            </div>

          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <!-- REFERENCES -->
  <section class="content-box box-3" id="references">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">References</h2>
          <div style="text-align:justify">
            <div style="text-align: justify; padding-top: 1.5em">
              <p>[1.] Marius Cordts et al. “The Cityscapes Dataset for Semantic Urban Scene Understanding”. In: Proc. of the IEEE
              Conference on Computer Vision and Pattern Recognition(CVPR). 2016. </p>
              <p> [2] Agrim Gupta et al. “Characterizing and Improving Stability in Neural Style Transfer”. In:CoRR abs/1705.02092
                (2017). arXiv: 1705 . 02092. URL: http: // arxiv. org / abs /1705.02092.</p>
              
              <p> [3] Andreas Rossler et al. “FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human
                Faces”.In:arXiv(2018).</p>
              
              <p> [4] Manuel Ruder, Alexey Dosovitskiy, and ThomasBrox. “Artistic style transfer for videos”. In: CoRR abs/1604.08610
                (2016). arXiv: 1604 . 08610. URL: </p>
              
              <p> [5] Ting-Chun Wang et al. “Video-to-video synthesis”. In:arXiv preprint arXiv:1808.06601(2018).</p>
              
              <p> [6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The
                Cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern
                Recognition (CVPR), 2016. </p>
              
              <p> [7] K. Aberman, M. Shi, J. Liao, D. Lischinski, B. Chen, and D. Cohen-Or. Deep video-based performance cloning.
                arXiv preprint arXiv:1808.06847, 2018.</p>
              
              <p> [8] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-person 2D pose estimation using part affinity fields.
                In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p>
              
              <p> [9] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In IEEE
                Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p>
              
              <p> [10] X. Liang, L. Lee, W. Dai, and E. P. Xing. Dual motion GAN for future-flow embedded video prediction. In
                Advances in Neural Information Processing Systems (NIPS), 2017.</p>
              
              <p> [11] A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier GANs. In International
                Conference on Machine Learning (ICML), 2017.</p>
              
              <p> [12] R. Yanagi, R. Togo, T. Ogawa and M. Haseyama, "Scene Retrieval for Video Summarization Based on Text-to-Image
                gan," 2019 IEEE International Conference on Image Processing (ICIP), Taipei, Taiwan, 2019, pp. 1825-1829.</p>
              
              <p> [13] C. Yang, Z.Wang, X. Zhu, C. Huang, J. Shi, and D. Lin. Pose guided human video generation. In European
                Conference on Computer Vision (ECCV), 2018.</p>
              
              <p> [14] X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin, and R. Yang. The ApolloScape dataset for
                autonomous driving. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</p>
              
              </p>[15] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In
              IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p>

            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  </div>
  <div style="text-align: right">
    <p> Theme Credits: <a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/">OSVOS Website</a></p>
  </div>
</body>

</html>
