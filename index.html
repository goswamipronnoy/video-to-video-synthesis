<!DOCTYPE html>

<html lang="en">

<head>

  <!-- Basic Page Needs
  ================================================== -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116539120-3"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-116539120-3');
  </script>

  <meta charset="utf-8">
  <title>Video to Video Synthesis</title>
  <meta name="description" content="Video to Video Synthesis">
  <!-- <meta name="author" content="CVL"> -->

  <!-- Mobile Specific Metas
	================================================== -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="css/materialize.min.css" media="screen,projection" />

  <!-- CSS
	================================================== -->
  <link rel="stylesheet" href="css/zerogrid.css">
  <link rel="stylesheet" href="css/style.css">

  <!-- Custom Fonts -->
  <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300" rel="stylesheet">


  <link rel="stylesheet" href="css/menu.css">
  <script src="js/jquery1111.min.js" type="text/javascript"></script>
  <script src="js/script.js"></script>

  <script type="text/javascript" src="js/materialize.min.js"></script>

  <!-- Add smooth transition -->
  <script>
    $(document).ready(function () {
      // Add smooth scrolling to all links
      $("a").on('click', function (event) {

        // Make sure this.hash has a value before overriding default behavior
        if (this.hash !== "") {
          // Prevent default anchor click behavior
          event.preventDefault();

          // Store hash
          var hash = this.hash;

          // Using jQuery's animate() method to add smooth page scroll
          // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
          $('html, body').animate({
            scrollTop: $(hash).offset().top
          }, 800, function () {

            // Add hash (#) to URL when done scrolling (default click behavior)
            window.location.hash = hash;
          });
        } // End if
      });
    });
  </script>

  <style type="text/css">
    /*********************************
    The list of publication items
    *********************************/
    /* The list of items */
    .biblist {}

    /* The item */
    .biblist li {}

    /* You can define custom styles for plstyle field here. */


    /*************************************
     The box that contain BibTeX code
     *************************************/
    div.noshow {
      display: none;
    }

    div.bibtex {
      margin-right: 0%;
      margin-top: 1.2em;
      margin-bottom: 1.3em;
      border: 1px solid silver;
      padding: 0.3em 0.5em;
      background: #ffffee;
    }

    div.bibtex pre {
      font-size: 75%;
      overflow: auto;
      width: 100%;
    }


    .content-subhead {
      color: black
    }

    .btn-large {
      text-transform: none;
      background-color: #AAAAAA;
    }

    .round {
      width: 100%;
      padding-top: 100%;
      overflow: hidden;
      position: relative;
      border-radius: 50%;
    }

    img.roundimg {
      position: absolute;
      top: 50%;
      left: 50%;
      min-width: 100%;
      height: 100%;
      transform: translate(-50%, -50%);
      transition: 1s ease;
    }
  </style>

  <script type="text/javascript">
 < !--
      // Toggle Display of BibTeX
      function toggleBibtex(articleid) {
        var bib = document.getElementById('bib_' + articleid);
        if (bib) {
          if (bib.className.indexOf('bibtex') != -1) {
            bib.className.indexOf('noshow') == -1 ? bib.className = 'bibtex noshow' : bib.className = 'bibtex';
          }
        } else {
          return;
        }
      }
    -->
  </script>
</head>

<body>
  <div class="wrap-body">

    <!--////////////////////////////////////Header-->
    <header class="zerogrid">
      <div id='cssmenu' class="align-center">
        <ul>
          <li class="active"><a href='index.html'><span>Home</span></a></li>
          <li><a href='#about'><span>About</span></a></li>
          <li><a href='#problemFormulation'><span>Problem Formulation</span></a></li>
          <li><a href='#background'><span>Background</span></a></li>
          <li><a href='#approach'><span>Approach</span></a></li>
          <li><a href='#results'><span>Results</span></a></li>
          <li><a href='#analysis'><span>Analysis and Benchmarks</span></a></li>
          <li><a href='#datasets'><span>Datasets</span></a></li>
          <li><a href='#training'><span>Training</span></a></li>
          <li><a href='#team'><span>Team</span></a></li>
          <li><a href='#timeline'><span>Timeline</span></a></li>
          <li><a href='#references'><span>References</span></a></li>
        </ul>
      </div>
    </header>


    <!--////////////////////////////////////Container-->
    <section id="container">
      <div class="wrap-container">
        <!-----------------content-box-2-------------------->
        <section class="content-box box-2">
          <div class="zerogrid">
            <div class="row wrap-box">
              <!--Start Box-->
              <div class="header">
                <hr class="line-2">
                <h1>Video to Video Synthesis</h1>
                <span style="line-height: 1.2">Our project aims to implement NVIDIA's Video-to-Video Synthesis research
                  paper, the goal of the research is to learn a mapping function from an input source video
                  (sequences of semantic segmentation masks) to an output photo-realistic video that precisely depicts
                  the content of the source video.</span>
              </div>
              <div class="row">
                <div class="col-1-2 t-center">
                  <img src="images/vid-gifs/cityscapes_change_labels.gif" style="width:98%" />
                </div>
                <div class="offsetonlyonmobile"></div>
                <div class="col-1-2 t-center">
                  <img src="images/vid-gifs/cityscapes_change_styles.gif" style="width:98%" />
                </div>
              </div>
              <article>
                <div class="row">


                  <a href="https://github.com/goswamipronnoy/video-to-video-synthesis" style="padding-right: 2em"
                    target="_blank"><img src="./images/github.png" alt="slides" width="
                  30" height="60"></img>Repository </a>
                  <a href="https://docs.google.com/presentation/d/1p7FVbR2tJ86ROuZO9RSLGC6lg06aDLjmBMRPKsLu2F0/edit?usp=sharing"
                    style="padding-right: 2em" target="_blank"><img src="./images/google_slides.png" alt="slides"
                      width="30" height="60"></img> Google Slides </a>
                  <a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf" style="padding-right: 2em"
                    target="_blank"><img src="./images/cvf.jpg" alt="slides" width="30" height="60"></img> Paper </a>
                </div>
              </article>
            </div>
          </div>
        </section>
      </div>
    </section>





    <section class="content-box box-3" id="about">
      <div class="zerogrid">
        <article>
          <div class="row t-center" style="line-height: 1.5">
            <hr class="line-2">
            <h2 style="margin-bottom:20px">About</h2>
            <div>
              <div style="text-align:justify; padding-top: 1em; padding-bottom: 2em;">
                <span>
                  The capability to model and recreate the dynamics of our visual world is essential to building
                  intelligent agents.
                  Apart from purely scientific interests, learning to synthesize continuous visual experiences has a
                  wide range of
                  applications in computer vision, robotics, and computer graphics. For example, in model-based
                  reinforcement learning
                  , a video synthesis model finds use in approximating visual dynamics of the world for training the
                  agent with less
                  amount of real experience data. Using a learned video synthesis model, one can generate realistic
                  videos without
                  explicitly specifying scene geometry, materials, lighting, and their dynamics, which would be
                  cumbersome but necessary
                  when using standard graphics rendering techniques.
                </span>
              </div>
              <div style="text-align:left">
                <h2 style="margin-bottom:20px; text-align:left">Motivation "TODO"</h2>
                <ul>
                  <li>1. Availability of Large Datasets [DAVIS, Youtube-VOS].</li>
                  <li>2. Availability of fast compute resources [GPU’s].</li>
                  <li>3. Vast number of applications [Video Editing, Augmented Reality].</li>
                </ul>
              </div>
            </div>
          </div>
        </article>
      </div>
  </div>
  </section>

  <section class="content-box box-3" id="problemFormulation">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Problem Formulation</h2>
          <div style="text-align:justify">
            <div><b>Given: </b>Aligned input and output videos.</div>
            <div>
              <b>Task: </b>Learn to map input videos to the output domain at the test time:
              <div style="text-align: justify; padding-left: 2.5em">
                <ul>
                  <li>1. Carefully designed Generator and Discriminator Networks.</li>
                  <li>2. Novel spatio-temporal learning objective function.</li>
                  <li>3. Method to multimodal video synthesis.</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <section class="content-box box-3" id="background">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Background</h2>
          <div style="text-align:justify">
            <div class="zerogrid">
              <article>
                <div style="line-height: 1">
                  <hr class="line-3">
                  <h3 style="margin-bottom:20px; padding-top: 1em; ">1. Background: Generative Adversarial Networks</h3>
                  <div style="text-align:justify">
                    <div class="row" style="text-align:center; padding-top:2em">
                      <img width="600" height="400" src="images/GAN.png"></img>
                      <figcaption style="padding-top: 1.5em"><b>Figure: A typical Generative Adversarial Networks
                          Pipeline</b></figcaption>
                      <div style="padding-top: 2em">
                        <div class="row" style="text-align:center; padding-top:2em">
                          <img width="600" height="400" src="images/Gan_eq.png"></img>
                          <div style="padding-top: 2em">
                          </div>
                        </div>
              </article>
              <article>
                <div style="line-height: 1">
                  <hr class="line-3">
                  <h3 style="margin-bottom:20px; padding-top: 1em; ">2. Background: Conditional Generative Adversarial
                    Networks</h3>
                  <div style="text-align:justify">
                    <div class="row" style="text-align:center; padding-top:2em">
                      <img width="600" height="400" src="images/conditionalGAN.png"></img>
                      <figcaption style="padding-top: 1.5em"><b>Figure: A typical Conditional Generative Adversarial
                          Networks Pipeline</b></figcaption>
                      <div style="padding-top: 2em">
                        <div class="row" style="text-align:center; padding-top:2em">
                          <img width="600" height="400" src="images/ConditionalGan_eq.png"></img>
                          <div style="padding-top: 2em">
                          </div>
                        </div>
			  </article>
			  <article>
					<div style="line-height: 1">
					  <hr class="line-3">
					  <h3 style="margin-bottom:20px; padding-top: 1em; ">3. Background: Image-to-Image Translation
						Networks</h3>
						<div class="row">
							<div class="col-1-2 t-center">
								<img src="images/im1.png" style="width:98%" />
								<figcaption style="padding-top: 1.5em"><b>Figure: "TODO"</b></figcaption>
							</div>
							<div class="col-1-2 t-center">
								<img src="images/im2.png" style="width:100%" />
								<figcaption style="padding-top: 1.5em"><b>Figure: "TODO"</b></figcaption>
							</div>	
						</div>
						<div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
							<h5><b>If we can accurately solve the problem for images, why not apply the same technique to videos?</b></h5>
						</div>
						<div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
							<h5 style="color:#5e8af2"><b>Issue: Temporal Coherence</b></h5>
						</div>
						<div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
							<img src="images/cityscapes_comparison.gif" style="width:100%" />
              <figcaption style="padding-top: 1.5em">Figure: Top Left: <b>Input Segmentation Mask</b>,
                                                                Top Right: <b>pix2pix</b>, 
                                                                Bottom Left: <b>COVST</b>, 
                                                                Bottom Right: <b>Video-to-Video</b></figcaption>
            </div>
            <div style="text-align:center">
              <p>
              Note: Here both pix2pix and COVST struggles to maintain temporal coherence while video-to-video synthesis gracefully
              handles it.
              </p>
				</article>
				</article>
            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>


  <!--APPROACH SECTION-->
  <section class="content-box box-3" id="approach">
		<div class="zerogrid">
		  <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Approach</h2>
          <div>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em; padding-bottom: 1.5em;">
            <h5 style="color:#5e8af2"><b>Add a Video Discriminator</b></h5>
          </div>
          <span>
            Video Discriminator : Ensure that consecutive output frames resemble temporal dynamics of a real video given the same Optical Flow.
          </span>
          </div>
        </div>

        <div style="line-height: 1">
          <hr class="line-3">
          <h3 style="margin-bottom:20px; padding-top: 1em; padding-left: 0em ">1. Conventions</h3>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/conventions.png" style="width:60%;" />
          </div>
        </div>

        <div style="line-height: 1">
          <hr class="line-3">
          <h3 style="margin-bottom:20px; padding-top: 1em; padding-left: 0em ">2. Goal</h3>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/goal_statement.png" style="width:80%;" />
          </div>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/goal.png" style="width:20%;" />
            <figcaption style="padding-top: 1.5em">Figure:"TODO"</figcaption>
          </div>
        </div>

        <div style="line-height: 1">
          <hr class="line-3">
          <h3 style="margin-bottom:20px; padding-top: 1em; padding-left: 0em ">3. Sequential Generator</h3>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/seqGen.png" style="width:40%;" />
            <figcaption style="padding-top: 1.5em">Figure:"TODO"</figcaption>
          </div>
          <div style=" padding-left: 24.5em; padding-top: 2em">
            <img src="images/sequentialGen.png" style="width:60%;" />
            <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
          </div>
          <div style="text-align: center; padding-top: 2em">
            <span>
            <b>Intution: </b>Wrap the current frame using optical flow from the current frame to generate an estimate of the next frame.
            </span>
          </div>
          <div style=" padding-left: 24.5em; padding-top: 2em">
            <img src="images/flow_eq.png" style="width:60%;" />
            <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
          </div>
          <div style=" padding-left: 24.5em; padding-top: 2em">
            <img src="images/flow_loss.png" style="width:60%;" />
            <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
          </div>
        </div>
        

        <div style="line-height: 1">
          <hr class="line-3">
          <h3 style="margin-bottom:20px; padding-top: 1em; padding-left: 0em ">4. Conditional Image Discriminator</h3>
          <div style="text-align:justify">
            <p>
              Ensure's the output frame resembles a real image.
            </p>
          </div>
          <div class="row">
            <div class="col-1-2 t-center">
              <img src="images/conditional_dis.png" style="width:70%; height:300px" />
              <figcaption style="padding-top: 1.5em"><b>Figure: "TODO"</b></figcaption>
            </div>
            <div class="col-1-2 t-center">
              <img src="images/image_dis.png" style="width:100%" />
              <figcaption style="padding-top: 1.5em"><b>Figure: "TODO"</b></figcaption>
            </div>	
          </div>
        </div>  
        <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
          <img src="images/image_loss.png" style="width:70%;" />
          <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
        </div>

        <div style="line-height: 1">
          <hr class="line-3">
          <h3 style="margin-bottom:20px; padding-top: 1em; padding-left: 0em ">5. Conditional Video Discriminator</h3>
          <div style="text-align:justify">
            <p>
            Ensure's that consecutive output frames resemble the temporal dynamics of a real video given the same optical flow.
            </p>
          </div>
          <div class="row">
            <div class="col-1-2 t-center">
              <img src="images/discriminator.gif" style="width:70%; height:300px" />
              <figcaption style="padding-top: 1.5em"><b>Figure: "TODO"</b></figcaption>
            </div>
            <div class="col-1-2 t-center">
              <img src="images/video_dis.png" style="width:100%" />
              <figcaption style="padding-top: 1.5em"><b>Figure: "TODO"</b></figcaption>
            </div>	
          </div>
        </div>  
        <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
          <img src="images/video_loss.png" style="width:80%;" />
          <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
        </div>

        <div style="line-height: 1">
          <hr class="line-3">
          <h3 style="margin-bottom:20px; padding-top: 1em; padding-left: 0em ">6. Learning Objective</h3>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/learning_obj.png" style="width:70%;" />
            <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
          </div>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/learning.gif" style="width:60%; height:550px" />
            <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
          </div>

        <div style="line-height: 1">
          <hr class="line-3">
          <h3 style="margin-bottom:20px; padding-top: 1em; padding-left: 0em ">7. Genearting High-Res Videos</h3>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/high_res1.png" style="width:70%;" />
            <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
          </div>
          <div style="text-align: center; padding-left: 2.5em; padding-top: 2em">
            <img src="images/high_res2.png" style="width:70%; " />
            <figcaption style="padding-top: 1.5em">Figure: "TODO"</figcaption>
        </div>

      </article>
	  </div>
	</section>


  <!--RESULTS SECTION-->
  <section class="content-box box-3" id="results">
    <div class="zerogrid">
      <div class="row wrap-box t-center" style="margin-left:10%;margin-right:10%;">
        <!--Start Box-->
        <hr class="line-2">
        <h2 style="margin-bottom:20px">Results</h2>
        <div class="row">
          <div class="col-1-5 t-center">
            <img src="images/1.jpg" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/2.jpg" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/16.jpg" style="width:98%" />
          </div>
          <div class="offsetonlyonmobile">
          </div>
          <div class="col-1-5 t-center">
            <img src="images/20.jpg" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/30.jpg" style="width:98%" />
          </div>
        </div>

        <div class="row">
          <div class="col-1-5 t-center">
            <img src="images/real_1.png" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_2.png" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_16.png" style="width:98%" />
          </div>
          <div class="offsetonlyonmobile">
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_20.png" style="width:98%" />
          </div>
          <div class="col-1-5 t-center">
            <img src="images/real_30.png" style="width:98%" />
          </div>
        </div>
        <figcaption style="padding-top:0.5em">Figure: Original Frames and their segmentation Masks for the Cityscapes
          dataset.</figcaption>
      </div>
    </div>

    <div style="padding-right: 2em; padding-top:1em; text-align:center">
      <img width="1024" height="720" src="images/result_video.gif"></img>
      <figcaption style="padding-top:2em">Figure: Video-to-video synthesis ouptut for the Cityscapes dataset.
      </figcaption>
    </div>
  </section>

  <!-- ANALYSIS AND BENCHMARKS -->
  <section class="content-box box-3" id="analysis">
    <div class="zerogrid">
      <article>
        <div class="row wrap-box t-center" style="margin-left:10%;margin-right:10%;">
          <!--Start Box-->
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Analysis and Benchmarks</h2>
          <div style="text-align:justify">
            <p>We used two different pre-trained video recognition CNNs in our evaluation: I3D [9] and ResNeXt [15]. We
              have re-used the benchmarks and analysis for the Human Preference Score
              from the research paper because we were not able to use the Amazon Mechanical Turk (AMT) platform.
              Additionally, we could not get participants for the AMT benchmarking.
            </p>

            <div style="line-height: 1">
              <hr class="line-3">
              <h3 style="margin-bottom:20px; padding-top: 1em; text-align: left;">Evaluation Metrics</h3>
            </div>

            <h4>1. Human Performance Score</h4>
            <div style="text-align: justify; padding-left: 2.5em; padding-top: 0.8em">
              <ul>
                <li> (a.) It is a human subjective test for evaluating quality of synthesized video.</li>
                <li> (b.) The research paper uses Amazon Mechanical Turk (AMT).</li>
                <li> (c.) The worker is first shown two videos at a time (results synthesized by two different
                  algorithms).</li>
                <li> (d.) The worker is then asked which one looks more like a video captured by a real camera.</li>
              </ul>
            </div>

            <h4 style="padding-top: 2em;">2. Fréchet Inception Distance (FID)</h4>
            <div style="text-align: justify; padding-left: 2.5em; padding-top: 0.8em; padding-bottom: 2em;">
              <ul>
                <li> (a.) Uses a pre-trained video recognition CNN as a feature extractor after removing the last few
                  layers from the network.</li>
                <li> (b.) For each video, extract a spatio-temporal feature map with this CNN.</li>
                <li> (c.) Compute Mean µ˜ and covariance Σ˜ for the feature vectors.</li>
              </ul>
              <p> The FID is then calculated as follows: </p>
              <div style="align-content: center; padding-left: 15em;"><img width="320" height="180"
                  src="images/fidEqn.png"></img></div>



            </div>

            <div style="line-height: 1">
              <hr class="line-3">
              <h3 style="margin-bottom:20px; padding-top: 1em; text-align: left;">Benchmarking Results</h3>
            </div>

            <div style="padding-right: 2em; padding-top:0.5em; text-align:center">
              <img width="900" height="600" src="images/analysis1.png"></img>
              <figcaption style="padding-top:0.5em">Table 1: Comparison between competing video-to-video synthesis
                approaches based on FID for Cityscapes.</figcaption>
            </div>

            <div style="padding-right: 2em; padding-top:1em; text-align:center">
              <img width="900" height="600" src="images/analysis2.png"></img>
              <figcaption style="padding-top:0.5em; padding-bottom: 2em;">Table 2: Comparison between competing
                video-to-video synthesis
                approaches based on Human Preference Score for Cityscapes.</figcaption>
            </div>
          </div>

          <div style="line-height: 1">
            <hr class="line-3">
            <h3 style="margin-bottom:20px; padding-top: 1em; text-align: left;">Graphs depicting the loss functions</h3>
          </div>
          <div style="padding-right: 2em; padding-top: 0.8em; text-align:center">
            <img width="900" height="600" src="images/loss_generator.png"></img>
            <figcaption style="padding-top:0.5em">Figure: Plot depicting the Generator Loss</figcaption>
          </div>
          <div style="padding-right: 2em; padding-top:2em; text-align:center">
            <img width="840" height="560" src="images/loss_discriminator.png"></img>
            <figcaption style="padding-top:0.5em">Figure: Plot depicting the Discriminator Loss</figcaption>
          </div>

        </div>
      </article>
    </div>
  </section>

  <!-- DATASETS -->
  <section class="content-box box-3" id="datasets">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Datasets</h2>
          <div style="text-align:justify">
            <div style="text-align: justify; padding-top: 1.5em">
              <p>The paper uses three different datasets to train their model. We specifically apply a pre-trained
                segmentation algorithm to
                get the corresponding semantic maps on the Cityscapes dataset. We apply a pre-trained segmentation
                algorithm to get the corresponding semantic maps (train_A) and instance maps
                (train_inst). DensePose/OpenPose is applied to their curated dataset consisting of random dancing videos
                found on YouTube to estimate the poses for each frame. We have used Python 3
                and PyTorch for obtaining our segmentation masks.</p>
              <p> The links to the available datasets are given below: </p>
            </div>
          </div>
          <div class="row" style="padding-top: 1.5em">
            <div class="col-1-2 t-center">
              <a class="waves-effect waves-light btn-large" style="width:90%" href="https://www.cityscapes-dataset.com/"
                target="_blank">Cityscapes</a>
            </div>
            <div class="col-1-2 t-center">
              <a class="waves-effect waves-light btn-large" style="width:90%"
                href="http://niessnerlab.org/projects/roessler2018faceforensics.html" target="_blank">FaceForensics</a>
            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <!-- LIVE TRAINING -->
  <section class="content-box box-3" id="training">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Training</h2>
          <div style="text-align:justify">
            <div style="padding-right: 2em; padding-top:1em; text-align:center">
              <img width="600" height="400" src="images/training.gif"></img>
              <figcaption style="padding-top:0.5em">Figure: Live training demonstration using the TensorBoard
              </figcaption>
            </div>

          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <!--TEAM-->
  <section class="content-box box-3" id="team">
    <div class="zerogrid">
      <div class="row wrap-box t-center" style="margin-left:10%;margin-right:10%;">
        <!--Start Box-->
        <hr class="line-2">
        <h2 style="margin-bottom:20px">Team</h2>
        <article>
          <div class="row">
            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/guptasaksham/" target="_blank"><img src="images/saksham.jpg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/guptasaksham/" target="_blank">
                    <h3 style="margin: 20px;">Saksham Gupta</h3>
                  </a>
                </div>
              </div>
            </div>

            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/pronnoygoswami/" target="_blank"><img src="images/pronnoy.jpeg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/pronnoygoswami/" target="_blank">
                    <h3 style="margin: 20px;">Pronnoy Goswami</h3>
                  </a>
                </div>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/parth-vora/" target="_blank"><img src="images/parth.jpg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/parth-vora/" target="_blank">
                    <h3 style="margin: 20px;">Parth Vora</h3>
                  </a>
                </div>
              </div>
            </div>

            <div class="column">
              <div class="col-1-2 t-center">
                <div class="wrap-col">
                  <a href="https://www.linkedin.com/in/mohak-bheda/" target="_blank"><img src="images/mohak.jpg"
                      width=200px height=200px style="border-radius: 50%" /></a>
                  <a href="https://www.linkedin.com/in/mohak-bheda/" target="_blank">
                    <h3 style="margin: 20px;">Mohak Bheda</h3>
                  </a>
                </div>
              </div>
            </div>
          </div>
        </article>
      </div>
    </div>
  </section>


  <!-- TIMELINE -->
  <section class="content-box box-3" id="TimeLine">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">Timeline (TODO: @Parth - add updated image)</h2>
          <div style="text-align:justify">
            <div style="padding-right: 2em; padding-top:1em; text-align:center">
              <img width="900" height="600" src="images/timeline.png"></img>
              <figcaption style="padding-top:0.5em">Figure: Timeline of Milestones and Deliverables</figcaption>
            </div>

          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  <!-- REFERENCES -->
  <section class="content-box box-3" id="references">
    <div class="zerogrid">
      <article>
        <div class="row t-center" style="line-height: 1.5">
          <hr class="line-2">
          <h2 style="margin-bottom:20px">References</h2>
          <div style="text-align:justify">
            <div style="text-align: justify; padding-top: 1.5em">
              <p>[1.] Marius Cordts et al. “The Cityscapes Dataset for Semantic Urban Scene Understanding”. In: Proc. of
                the IEEE
                Conference on Computer Vision and Pattern Recognition(CVPR). 2016. </p>
              <p> [2] Agrim Gupta et al. “Characterizing and Improving Stability in Neural Style Transfer”. In:CoRR
                abs/1705.02092
                (2017). arXiv: 1705 . 02092. URL: http: // arxiv. org / abs /1705.02092.</p>

              <p> [3] Andreas Rossler et al. “FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human
                Faces”.In:arXiv(2018).</p>

              <p> [4] Manuel Ruder, Alexey Dosovitskiy, and ThomasBrox. “Artistic style transfer for videos”. In: CoRR
                abs/1604.08610
                (2016). arXiv: 1604 . 08610. URL: </p>

              <p> [5] Ting-Chun Wang et al. “Video-to-video synthesis”. In:arXiv preprint arXiv:1808.06601(2018).</p>

              <p> [6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B.
                Schiele. The
                Cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and
                Pattern
                Recognition (CVPR), 2016. </p>

              <p> [7] K. Aberman, M. Shi, J. Liao, D. Lischinski, B. Chen, and D. Cohen-Or. Deep video-based performance
                cloning.
                arXiv preprint arXiv:1808.06847, 2018.</p>

              <p> [8] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-person 2D pose estimation using part
                affinity fields.
                In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p>

              <p> [9] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.
                In IEEE
                Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p>

              <p> [10] X. Liang, L. Lee, W. Dai, and E. P. Xing. Dual motion GAN for future-flow embedded video
                prediction. In
                Advances in Neural Information Processing Systems (NIPS), 2017.</p>

              <p> [11] A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier GANs. In
                International
                Conference on Machine Learning (ICML), 2017.</p>

              <p> [12] R. Yanagi, R. Togo, T. Ogawa and M. Haseyama, "Scene Retrieval for Video Summarization Based on
                Text-to-Image
                gan," 2019 IEEE International Conference on Image Processing (ICIP), Taipei, Taiwan, 2019, pp.
                1825-1829.</p>

              <p> [13] C. Yang, Z.Wang, X. Zhu, C. Huang, J. Shi, and D. Lin. Pose guided human video generation. In
                European
                Conference on Computer Vision (ECCV), 2018.</p>

              <p> [14] X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin, and R. Yang. The ApolloScape
                dataset for
                autonomous driving. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</p>

              </p>[15] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep
              neural networks. In
              IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p>

            </div>
          </div>
        </div>
      </article>
    </div>
    </div>
  </section>

  </div>
  <div style="text-align: right">
    <p> Theme Credits: <a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/" target="_blank">OSVOS Website</a>
    </p>
  </div>
</body>

</html>
